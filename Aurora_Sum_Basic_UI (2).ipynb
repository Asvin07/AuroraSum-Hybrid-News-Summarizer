{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QfZJkrLwN-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7d8e3f-fac2-4c90-8244-6334ada281fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.3/51.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install gradio transformers sumy torch nltk evaluate rouge-score bert_score -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio transformers sumy torch nltk -q\n"
      ],
      "metadata": {
        "id": "OLPpW_Ib4WGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "\n",
        "# Hybrid Summarizer with optimized parameters and no reference dependency\n",
        "class HybridSummarizer:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"facebook/bart-large-cnn\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = BartForConditionalGeneration.from_pretrained(self.model_name)\n",
        "        self.text_rank = TextRankSummarizer()\n",
        "\n",
        "        self.best_params = {\n",
        "            \"max_length\": 82,\n",
        "            \"min_length\": 66,\n",
        "            \"num_key_sentences\": 5,\n",
        "            \"temperature\": 1.13,\n",
        "            \"length_penalty\": 1.06\n",
        "        }\n",
        "\n",
        "    def extract_key_sentences(self, text, num_sentences=3):\n",
        "        parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "        return \" \".join([str(s) for s in self.text_rank(parser.document, num_sentences)])\n",
        "\n",
        "    def summarize(self, text):\n",
        "        if not text.strip():\n",
        "            return \"Please enter some valid input text.\"\n",
        "\n",
        "        key_points = self.extract_key_sentences(text, self.best_params[\"num_key_sentences\"])\n",
        "        prompt = f\"[CNN] Generate a news summary from these key points: {key_points}\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "        if torch.cuda.is_available():\n",
        "            self.model = self.model.to(\"cuda\")\n",
        "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                num_beams=4,\n",
        "                max_length=self.best_params[\"max_length\"],\n",
        "                min_length=self.best_params[\"min_length\"],\n",
        "                temperature=self.best_params[\"temperature\"],\n",
        "                length_penalty=self.best_params[\"length_penalty\"],\n",
        "                do_sample=False,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Instantiate summarizer\n",
        "summarizer = HybridSummarizer()\n",
        "\n",
        "# Gradio Interface (only input and output)\n",
        "def summarize_text(text):\n",
        "    return summarizer.summarize(text)\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    # Fancy header with background gradient\n",
        "    gr.HTML(\"\"\"\n",
        "    <div style=\"background: linear-gradient(to right, #667eea, #764ba2); padding: 20px; border-radius: 12px; text-align: center;\">\n",
        "        <h1 style=\"color:white;\">AuroraSum Summarizer</h1>\n",
        "        <p style=\"color:white;\">Powered by BART + TextRank â€“ Clean, Accurate, and Lightning Fast</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Input area\n",
        "    with gr.Row():\n",
        "        input_box = gr.Textbox(label=\"ğŸ“ Input Article\", lines=10, placeholder=\"Paste your article or paragraph here...\")\n",
        "\n",
        "    # Button and Output\n",
        "    with gr.Row():\n",
        "        summarize_button = gr.Button(\"âœ¨ Summarize Now\")\n",
        "\n",
        "    output_box = gr.Textbox(label=\"ğŸ“„ Generated Summary\", lines=6)\n",
        "\n",
        "\n",
        "\n",
        "    # Bind button to summarization function\n",
        "    summarize_button.click(fn=summarize_text, inputs=input_box, outputs=output_box)\n",
        "\n",
        "# Launch the app\n",
        "demo.launch(share=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "DiHt_9yj5uTz",
        "outputId": "3d6539c5-65e9-4140-cbab-6b8408661cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f2d4afe1c25dfae4a9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f2d4afe1c25dfae4a9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "\n",
        "import evaluate\n",
        "import gradio as gr\n",
        "\n",
        "# Summarization Class\n",
        "class HybridSummarizer:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"facebook/bart-large-cnn\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = BartForConditionalGeneration.from_pretrained(self.model_name)\n",
        "        self.text_rank = TextRankSummarizer()\n",
        "\n",
        "        self.best_params = {\n",
        "            \"max_length\": 82,\n",
        "            \"min_length\": 66,\n",
        "            \"num_key_sentences\": 5,\n",
        "            \"temperature\": 1.13,\n",
        "            \"length_penalty\": 1.06\n",
        "        }\n",
        "\n",
        "    def extract_key_sentences(self, text, num_sentences=3):\n",
        "        parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "        return \" \".join(str(s) for s in self.text_rank(parser.document, num_sentences))\n",
        "\n",
        "    def summarize(self, text):\n",
        "        if not text.strip():\n",
        "            return \"Please enter valid input text.\"\n",
        "\n",
        "        key_points = self.extract_key_sentences(text, self.best_params[\"num_key_sentences\"])\n",
        "        prompt = f\"[CNN] Generate a news summary from these key points: {key_points}\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "        if torch.cuda.is_available():\n",
        "            self.model.to(\"cuda\")\n",
        "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                num_beams=4,\n",
        "                max_length=self.best_params[\"max_length\"],\n",
        "                min_length=self.best_params[\"min_length\"],\n",
        "                temperature=self.best_params[\"temperature\"],\n",
        "                length_penalty=self.best_params[\"length_penalty\"],\n",
        "                do_sample=False,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Evaluation Class\n",
        "class SummaryEvaluator:\n",
        "    def __init__(self):\n",
        "        self.rouge = evaluate.load(\"rouge\")\n",
        "        self.bertscore = evaluate.load(\"bertscore\")\n",
        "        self.meteor = evaluate.load(\"meteor\")\n",
        "\n",
        "    def evaluate(self, predicted, reference):\n",
        "        try:\n",
        "            rouge_scores = self.rouge.compute(predictions=[predicted], references=[reference])\n",
        "            bert_scores = self.bertscore.compute(predictions=[predicted], references=[reference], lang=\"en\")\n",
        "            meteor_score = self.meteor.compute(predictions=[predicted], references=[reference])\n",
        "        except Exception as e:\n",
        "            return {\"Error\": f\"Evaluation failed: {str(e)}\"}\n",
        "\n",
        "        return {\n",
        "            \"ROUGE-1\": round(rouge_scores[\"rouge1\"], 4),\n",
        "            \"ROUGE-2\": round(rouge_scores[\"rouge2\"], 4),\n",
        "            \"ROUGE-L\": round(rouge_scores[\"rougeL\"], 4),\n",
        "            \"METEOR\": round(meteor_score[\"meteor\"], 4),\n",
        "            \"BERTScore-F1\": round(bert_scores[\"f1\"][0], 4)\n",
        "        }\n",
        "\n",
        "# Initialize both\n",
        "summarizer = HybridSummarizer()\n",
        "evaluator = SummaryEvaluator()\n",
        "\n",
        "# Gradio Logic\n",
        "def summarize_and_evaluate(article, reference=\"\"):\n",
        "    summary = summarizer.summarize(article)\n",
        "    if reference.strip():\n",
        "        scores = evaluator.evaluate(summary, reference)\n",
        "        score_str = \"\\n\".join(f\"{k}: {v}\" for k, v in scores.items())\n",
        "    else:\n",
        "        score_str = \"âš ï¸ No reference summary provided. Evaluation skipped.\"\n",
        "\n",
        "    return summary, score_str\n",
        "\n",
        "# Gradio Interface\n",
        "gr.Interface(\n",
        "    fn=summarize_and_evaluate,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=10, label=\"Input Article or Paragraph\"),\n",
        "        gr.Textbox(lines=5, label=\"(Optional) Reference Summary\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Generated Summary\"),\n",
        "        gr.Textbox(label=\"Evaluation Scores\")\n",
        "    ],\n",
        "    title=\"AuroraSum: Summarization + Evaluation\",\n",
        "    description=\"Enter an article to generate a summary using BART + TextRank. Optionally add a reference summary to see evaluation scores like ROUGE, METEOR, and BERTScore.\"\n",
        ").launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "CfKuM54f6zyi",
        "outputId": "6f983bfa-e9c6-41d9-e0a5-5526ca6c7805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://81b54ee42eb642e94b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://81b54ee42eb642e94b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQWv1ecULo32"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}